# Text-Summarization-Using-Deep-Learning

The objective of this project is to create concise and fluent summaries for the Amazon Fine Food Reviews dataset by using abstractive summarization technique and deep neural networks. The data can be accessed from the following link:

Data: https://www.kaggle.com/snap/amazon-fine-food-reviews

Amazon Fine Food Reviews dataset consists of more than 500,000 rows having 10 columns, spanning over a period of more than ten years upto October 2012. Only first 100,000 rows are used for the model focussing on most relevant columns for experiment: Summary and Text. The dataset is further filtered such that the review text contains a maximum of forty word count and the summary contains up to five word count. This results in the dataset size of 57,861 reviews and summaries.

The neural network uses a Sequence-to-Sequence (Seq2Seq) model where the input is a long sequence of words and the output is a short summary of words based on the input sequence. The encoder-decoder architecture captures the idea of converting the source sequence into a continuous vector. The final state of the encoder is fed to the decoder to generate the target summary. LSTM (Long Short Term Memory) units are used in encoder-decoder model that not only process a single data unit, but a sequence of words while remembering the previous word to predict the next word in the sequence. The Attention Layer (https://github.com/thushv89/attention_keras/blob/master/src/layers/attention.py) gives importance to most relevant parts of the source sequence that results in the accurate target sequence.

The predicted summaries seem quite similar to the human-generated summaries and convey the salient features of the reviews. A sample of the review text, original summary and predicted summary generated by the model can be viewed in the code file.
